{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubeflow GPU Pipeline\n",
    "\n",
    "In this notebook, we are building a Kubeflow pipeline whihch will use the GPU to train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Clothing\n",
    "\n",
    "The Fashion MNIST dataset contains 70,000 grayscale images in 10 clothing categories. Each image is 28x28 pixels in size.\n",
    "You might have seen this example before as it's a popular classifier, well suited to learning how to use Tensorflow and machine learning.\n",
    "\n",
    "\n",
    "<b>Steps:</b>\n",
    "\n",
    "* Create a PVC.\n",
    "* Copy required files to PVC.\n",
    "* Submit the pipeline from notebook.\n",
    "\n",
    "\n",
    "\n",
    "First 2 step you need to perfrom before executing the notebook on k8s master host."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PVC\n",
    "\n",
    "This step we need to perfrom using the kubectl.\n",
    "\n",
    "Content of fashion-mnist-pvc.yaml\n",
    "\n",
    "```\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: fashion-mnist-pvc\n",
    "spec:\n",
    "  accessModes:\n",
    "  - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 2Gi\n",
    "```\n",
    "\n",
    "Create a pvc using kubectl.\n",
    "```\n",
    "kubectl create -f fashion-mnist-pvc.yaml -n kubeflow\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy required files to PVC.\n",
    "\n",
    "For copying the required files, first we need to download required file and then we need to copy those files to PV using a utility pod.\n",
    "\n",
    "you can download required files from below public links:\n",
    "* https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/data/fashion/t10k-images-idx3-ubyte.gz\n",
    "* https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/data/fashion/t10k-labels-idx1-ubyte.gz\n",
    "* https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/data/fashion/train-images-idx3-ubyte.gz\n",
    "* https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/data/fashion/train-labels-idx1-ubyte.gz\n",
    "\n",
    "<b>Commands:</b>\n",
    "```\n",
    "export http_proxy=http://web-proxy.corp.hpecorp.net:8080\n",
    "export https_proxy=http://web-proxy.corp.hpecorp.net:8080\n",
    "wget https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/data/fashion/t10k-images-idx3-ubyte.gz\n",
    "wget https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/data/fashion/t10k-labels-idx1-ubyte.gz\n",
    "wget https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/data/fashion/train-images-idx3-ubyte.gz\n",
    "wget https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/data/fashion/train-labels-idx1-ubyte.gz\n",
    "```\n",
    "\n",
    "\n",
    "<b>Content of utility pod.yaml</b>\n",
    "\n",
    "```\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "    name: dataaccess\n",
    "spec:\n",
    "    containers:\n",
    "    - name: alpine\n",
    "      image: alpine:latest\n",
    "      command: ['sleep', 'infinity']\n",
    "      volumeMounts:\n",
    "      - name: mypvc\n",
    "        mountPath: /data\n",
    "    volumes:\n",
    "    - name: mypvc\n",
    "      persistentVolumeClaim:\n",
    "        claimName: fashion-mnist-pvc\n",
    "```\n",
    "\n",
    "<b>Create a pod.</b>\n",
    "```\n",
    "kubectl create -f po.yaml -n kubeflow\n",
    "```\n",
    "\n",
    "\n",
    "<b>Copy downloaded files to PVC using pod.</b>\n",
    "```\n",
    "kubectl cp t10k-images-idx3-ubyte.gz dataaccess:/data -n kubeflow\n",
    "kubectl cp t10k-labels-idx1-ubyte.gz dataaccess:/data -n kubeflow\n",
    "kubectl cp train-images-idx3-ubyte.gz dataaccess:/data -n kubeflow\n",
    "kubectl cp train-labels-idx1-ubyte.gz dataaccess:/data -n kubeflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "from typing import NamedTuple\n",
    "from kfp import onprem\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ignore this step if you are running from Kubeflow Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezmllib.kubeflow.ezkfp import KfSession\n",
    "K = KfSession()\n",
    "client=K.kf_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "train() is python based function which will be used to train our model on GPU and read the dataset files from PVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_path, model_file)-> NamedTuple('output', [('mlpipeline_ui_metadata', 'UI_metadata'), ('mlpipeline_metrics', 'Metrics')]):\n",
    "    \n",
    "    import pickle\n",
    "    import json, os, gzip\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    from tensorflow.python import keras\n",
    "    \n",
    "    \n",
    "    #Loading dataset offline\n",
    "    data_folder = f'{data_path}'\n",
    "    files = [\n",
    "        'train-labels-idx1-ubyte.gz','train-images-idx3-ubyte.gz',\n",
    "        't10k-labels-idx1-ubyte.gz','t10k-images-idx3-ubyte.gz'\n",
    "    ]\n",
    "    paths = []\n",
    "    for fname in files:\n",
    "        paths.append(os.path.join(data_folder,fname))\n",
    "        \n",
    "    with gzip.open(paths[0],'rb') as lbpath:\n",
    "        y_train = np.frombuffer(lbpath.read(),np.uint8,offset=8)\n",
    "        \n",
    "    with gzip.open(paths[1],'rb') as imgpath:\n",
    "        X_train = np.frombuffer(imgpath.read(),np.uint8,offset=16).reshape(len(y_train),28,28)\n",
    "    \n",
    "    with gzip.open(paths[2],'rb') as lbpath:\n",
    "        y_test = np.frombuffer(lbpath.read(),np.uint8,offset=8)\n",
    "        \n",
    "    with gzip.open(paths[3],'rb') as imgpath:\n",
    "        X_test = np.frombuffer(imgpath.read(),np.uint8,offset=16).reshape(len(y_test),28,28)\n",
    "   \n",
    "    \n",
    "    #with tf.device('/GPU:0'):\n",
    "    # Download the dataset and split into training and test data from internet. \n",
    "    #fashion_mnist = keras.datasets.fashion_mnist\n",
    "    #(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "    \n",
    "    (train_images, train_labels), (test_images, test_labels) = (X_train,y_train),(X_test,y_test)\n",
    "        \n",
    "        \n",
    "    # Normalize the data so that the values all fall between 0 and 1.\n",
    "    train_images = train_images / 255.0\n",
    "    test_images = test_images / 255.0\n",
    "        \n",
    "        # Define the model using Keras.\n",
    "    model = keras.Sequential([\n",
    "            keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            keras.layers.Dense(128, activation='relu'),\n",
    "            keras.layers.Dense(10)\n",
    "        ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        \n",
    "    # Run a training job with specified number of epochs\n",
    "    model.fit(train_images, train_labels, epochs=10)\n",
    "        \n",
    "    # Evaluate the model and print the results\n",
    "    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "    print('Test accuracy:', test_acc)\n",
    "        \n",
    "    # Save the model to the designated \n",
    "    model.save(f'{data_path}/{model_file}')\n",
    "        \n",
    "        \n",
    "    # Save the test_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/test_data', 'wb') as f:\n",
    "        pickle.dump((test_images,test_labels), f)\n",
    "            \n",
    "    metadata = {\n",
    "            'outputs' : [{'type': 'web-app',\n",
    "                          'storage': 'inline',\n",
    "                          'source': \"<div>Done</div>\",\n",
    "                         }]\n",
    "        }\n",
    "        \n",
    "    metrics = {\n",
    "            'metrics': [{\n",
    "                'name': 'Accuracy',\n",
    "                'numberValue':  float(test_acc),\n",
    "            }, {\n",
    "                'name': 'Loss',\n",
    "                'numberValue':  float(test_loss),\n",
    "            }]}\n",
    "        \n",
    "    from collections import namedtuple\n",
    "    print_output = namedtuple('output', ['mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "    return print_output(json.dumps(metadata), json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction \n",
    "predict() is python based function which will be used to predict the type of image from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_path, model_file, image_number)-> NamedTuple('output', [('mlpipeline_ui_metadata', 'UI_metadata'), ('mlpipeline_metrics', 'Metrics')]):\n",
    "    \n",
    "    import pickle\n",
    "    import json, os, gzip\n",
    "    \n",
    "    import base64\n",
    "    from PIL import Image\n",
    "    from io import BytesIO\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "\n",
    "    import numpy as np\n",
    "    \n",
    "    # Load the saved Keras model\n",
    "    model = keras.models.load_model(f'{data_path}/{model_file}')\n",
    "\n",
    "    # Load and unpack the test_data\n",
    "    with open(f'{data_path}/test_data','rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    # Separate the test_images from the test_labels.\n",
    "    test_images, test_labels = test_data\n",
    "    # Define the class names.\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "    # Define a Softmax layer to define outputs as probabilities\n",
    "    probability_model = tf.keras.Sequential([model, \n",
    "                                            tf.keras.layers.Softmax()])\n",
    "\n",
    "    # See https://github.com/kubeflow/pipelines/issues/2320 for explanation on this line.\n",
    "    image_number = int(image_number)\n",
    "\n",
    "    # Grab an image from the test dataset.\n",
    "    selected_image = test_images[image_number]\n",
    "\n",
    "    # Add the image to a batch where it is the only member.\n",
    "    img = (np.expand_dims(selected_image,0))\n",
    "\n",
    "    # Predict the label of the image.\n",
    "    predictions = probability_model.predict(img)\n",
    "\n",
    "    # Take the prediction with the highest probability\n",
    "    prediction = np.argmax(predictions[0])\n",
    "\n",
    "    # Retrieve the true label of the image from the test labels.\n",
    "    true_label = test_labels[image_number]\n",
    "    \n",
    "    class_prediction = class_names[prediction]\n",
    "    confidence = 100*np.max(predictions)\n",
    "    actual = class_names[true_label]\n",
    "    \n",
    "    \n",
    "    with open(f'{data_path}/result.txt', 'w') as result:\n",
    "        result.write(\" Prediction: {} | Confidence: {:2.0f}% | Actual: {}\".format(class_prediction,\n",
    "                                                                                confidence,\n",
    "                                                                                actual))\n",
    "\n",
    "    PIL_image = Image.fromarray(np.uint8(selected_image * 255)).convert('RGB')                                                                            \n",
    "    buffered = BytesIO()\n",
    "    PIL_image.save(buffered, format=\"JPEG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "    metadata = {\n",
    "        'outputs' : [{\n",
    "          'type': 'web-app',\n",
    "          'storage': 'inline',\n",
    "          'source': '''<div>Input: <img width=\\\"200\\\" src=\\\"data:image/jpeg;base64,{}\\\"/></div><p>Prediction: {}</p>'''.format(img_str, actual),\n",
    "        }]\n",
    "      }\n",
    "    \n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': 'Confidence',\n",
    "          'numberValue': confidence,\n",
    "        }]}\n",
    "\n",
    "    from collections import namedtuple\n",
    "    print_output = namedtuple('output', ['mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "    return print_output(json.dumps(metadata), json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python function-based component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = comp.func_to_container_op(train, base_image='devsds/tensorflow:latest-gpu')\n",
    "predict_op = comp.func_to_container_op(predict, base_image='devsds/tensorflow:latest-gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining variable required for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PVC name \n",
    "PVC_NAME = 'fashion-mnist-pvc'\n",
    "\n",
    "# Path where dataset files are stored.\n",
    "DATA_PATH = '/mnt'\n",
    "\n",
    "# Model file name.\n",
    "MODEL_PATH='mnist_model.h5'\n",
    "\n",
    "# An integer representing an image from the test set that the model will attempt to predict the label for.\n",
    "IMAGE_NUMBER =  random.randint(0,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kubeflow Pipeline\n",
    "A pipeline that performs fashion MNIST model training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='Fashion MNIST Pipeline',\n",
    "   description='A pipeline that performs fashion MNIST model training and prediction.'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def mnist_container_pipeline(\n",
    "    data_path: str = DATA_PATH,\n",
    "    model_file: str = MODEL_PATH, \n",
    "    image_number: int = IMAGE_NUMBER\n",
    "):\n",
    "    \n",
    "    \n",
    "    # 2. Create MNIST training component.\n",
    "    mnist_training_container = train_op(data_path, model_file) \\\n",
    "                                    .apply(onprem.mount_pvc(PVC_NAME, 'local-storage', DATA_PATH)).set_gpu_limit(1)\n",
    "\n",
    "    # 3. Create MNIST prediction component.\n",
    "    mnist_predict_container = predict_op(data_path,\n",
    "                                         model_file,\n",
    "                                         image_number\n",
    "                                         ) \\\n",
    "                                    .apply(onprem.mount_pvc(PVC_NAME, 'local-storage', DATA_PATH)).set_gpu_limit(1)\n",
    "    \n",
    "    mnist_predict_container.after(mnist_training_container)\n",
    "    \n",
    "    # 4. Print the result of the prediction\n",
    "    mnist_result_container = dsl.ContainerOp(\n",
    "        name=\"print_prediction\",\n",
    "        image='library/bash:4.4.23',\n",
    "        arguments=['cat', f'{data_path}/result.txt']\n",
    "    ).apply(onprem.mount_pvc(PVC_NAME, 'local-storage', DATA_PATH))\n",
    "    \n",
    "    mnist_result_container.after(mnist_predict_container)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = mnist_container_pipeline\n",
    "experiment_name = 'fashion_minist_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\n",
    "    \"data_path\":DATA_PATH,\n",
    "    \"model_file\":MODEL_PATH,\n",
    "    \"image_number\": IMAGE_NUMBER\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## If you are running pipeline from Kubedirector Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezmllib.kubeflow.ezkfp import KfSession\n",
    "K = KfSession()\n",
    "client=K.kf_client()\n",
    "client.create_run_from_pipeline_func(\n",
    "    pipeline_func, \n",
    "    experiment_name=experiment_name, \n",
    "    run_name=run_name, \n",
    "    arguments=arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## If you are running pipeline from Kubeflow Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kfp.Client().create_run_from_pipeline_func(\n",
    "    pipeline_func, \n",
    "    experiment_name=experiment_name, \n",
    "    run_name=run_name, \n",
    "    arguments=arguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "devsds/kubeflow-kale@sha256:dcb5c4705651c6c142f7a4e7521020e4ef45118db29aacc2fca5d813283af90a",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": false,
   "steps_defaults": [],
   "volume_access_mode": "rwm",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
