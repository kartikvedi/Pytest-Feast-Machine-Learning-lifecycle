{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data.txt\n",
    "Apache Spark is an open-source processing engine that you can use to process Hadoop data. The following diagram shows the components involved in running Spark jobs. See Spark Cluster Mode Overview for additional component details.\n",
    "\n",
    "HPE Ezmeral Data Fabric supports the following types of cluster managers:\n",
    "Spark's standalone cluster manager\n",
    "YARN\n",
    "The configuration and operational steps for Spark differ based on the Spark mode you choose to install. The steps to integrate Spark with other components are the same when using either Standalone of YARN cluster mode, except where otherwise noted.\n",
    "This section provides documentation about configuring and using Spark with HPE Ezmeral Data Fabric, but it does not duplicate the Apache Spark documentation.\n",
    "\n",
    "You can also refer to additional documentation available on the Apache Spark Product Page.\n",
    "\n",
    "Getting Started with Spark Interactive Shell\n",
    "After you have a basic understanding of Apache Spark and have it installed and running on your cluster, you can use it to load datasets, apply schemas, and query data from the Spark interactive shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -mkdir dtap://TenantStorage/tmp\n",
    "!hadoop fs -copyFromLocal data.txt dtap://TenantStorage/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML=\"\"\"\n",
    "apiVersion: \"sparkoperator.hpe.com/v1beta2\"\n",
    "kind: SparkApplication\n",
    "metadata:\n",
    "  name: spark-wordcount-secure-dtap\n",
    "spec:\n",
    "  sparkConf:\n",
    "    # Note: If you are executing the application as a K8S user that MapR can verify,\n",
    "    #       you do not need to specify a spark.mapr.user.secret\n",
    "    #spark.mapr.user.secret: spark-user-secret\n",
    "    # Note: You do not need to specify a spark.eventLog.dir\n",
    "    #       it will be auto-generated with the pattern \"maprfs:///apps/spark/<namespace>\"\n",
    "    #spark.eventLog.dir: \"maprfs:///apps/spark/sampletenant\"\n",
    "\n",
    "    # DTAP configuration\n",
    "    spark.hadoop.fs.dtap.impl: \"com.bluedata.hadoop.bdfs.Bdfs\"\n",
    "    spark.hadoop.fs.AbstractFileSystem.dtap.impl: \"com.bluedata.hadoop.bdfs.BdAbstractFS\"\n",
    "    spark.hadoop.fs.dtap.impl.disable.cache: \"false\"\n",
    "    spark.driver.extraClassPath: \"/opt/bdfs/bluedata-dtap.jar\"\n",
    "    spark.executor.extraClassPath: \"/opt/bdfs/bluedata-dtap.jar\"\n",
    "  type: Java\n",
    "  sparkVersion: 3.1.2\n",
    "  mode: cluster\n",
    "  # adding dtap connector to spark classpath\n",
    "  deps:\n",
    "    jars:\n",
    "      - local:///opt/bdfs/bluedata-dtap.jar\n",
    "  image: gcr.io/mapr-252711/spark-3.1.2:202202161825P150\n",
    "  imagePullPolicy: Always\n",
    "  mainClass: org.apache.spark.examples.JavaWordCount\n",
    "  mainApplicationFile: \"local:///opt/mapr/spark/spark-3.1.2/examples/jars/spark-examples_2.12-3.1.2.3-eep-800.jar\"\n",
    "  restartPolicy:\n",
    "    type: Never\n",
    "  arguments:\n",
    "    - dtap://TenantStorage/tmp/data.txt\n",
    "  imagePullSecrets:\n",
    "    - imagepull\n",
    "  driver:\n",
    "    cores: 1\n",
    "    coreLimit: \"1000m\"\n",
    "    memory: \"512m\"\n",
    "    labels:\n",
    "      version: 3.1.2\n",
    "      hpecp.hpe.com/dtap: hadoop2 # enabling dtap side-car container for driver pod\n",
    "    # Note: You do not need to specify a serviceAccount\n",
    "    #       it will be auto-generated referencing the pre-existing \"hpe-<namespace>\"\n",
    "    serviceAccount: hpe-testing\n",
    "  executor:\n",
    "    cores: 1\n",
    "    coreLimit: \"1000m\"\n",
    "    instances: 2\n",
    "    memory: \"512m\"\n",
    "    labels:\n",
    "      version: 3.1.2\n",
    "      hpecp.hpe.com/dtap: hadoop2 # enabling dtap side-car container for executor pods\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dtap-wordcount.yaml\", \"w+\") as f:\n",
    "    f.write(YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezmllib.spark import submit, delete, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(yaml_path=\"dtap-wordcount.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "timestamp=$(date +%s --date=\"$giveDate 60 minutes\")\n",
    "while true ; do\n",
    "    current_timestamp=$(date +%s)\n",
    "    POD=$(kubectl get pods --no-headers -o custom-columns=\":metadata.name\" | grep spark-wordcount-secure-dtap-driver)\n",
    "    if [ -n \"$POD\" ]; then\n",
    "        STATUS=$(kubectl get pod $POD --no-headers | awk '{ print $3 }')\n",
    "        if [ \"$STATUS\" == \"Error\" ]; then\n",
    "            echo \"Test Failed.\"\n",
    "            break\n",
    "        fi\n",
    "        if [ \"$STATUS\" == \"ImagePuErrorllBackOff\" ]; then\n",
    "            echo \"Test Failed.\"\n",
    "            break\n",
    "        fi\n",
    "        if [ \"$STATUS\" == \"Completed\" ]; then\n",
    "            echo \"Test Passed.\"\n",
    "            break\n",
    "        fi\n",
    "    sleep 20s\n",
    "    fi\n",
    "    \n",
    "    if [ \"$current_timestamp\" -gt \"$timestamp\" ]; then\n",
    "        echo \"Test Failed.\"\n",
    "        break\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f dtap-wordcount.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
